{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0edc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess MNIST\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5  # Scale to [-1, 1]\n",
    "X_train = np.expand_dims(X_train, axis=-1)  # (N, 28, 28, 1)\n",
    "\n",
    "latent_dim = 100\n",
    "\n",
    "# Generator\n",
    "def build_generator():\n",
    "    model = Sequential([\n",
    "        Dense(7*7*256, use_bias=False, input_shape=(latent_dim,)),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(),\n",
    "        Reshape((7, 7, 256)),\n",
    "        Conv2DTranspose(128, (5,5), strides=(1,1), padding='same', use_bias=False),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(),\n",
    "        Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(),\n",
    "        Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[28,28,1]),\n",
    "        LeakyReLU(0.2),\n",
    "        Dropout(0.3),\n",
    "        Conv2D(128, (5,5), strides=(2,2), padding='same'),\n",
    "        LeakyReLU(0.2),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Instantiate models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Print architectures\n",
    "print(\"Generator architecture:\")\n",
    "generator.summary()\n",
    "print(\"\\nDiscriminator architecture:\")\n",
    "discriminator.summary()\n",
    "\n",
    "# Compile models\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "discriminator.trainable = False\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "valid = discriminator(img)\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# Training\n",
    "epochs = 10000\n",
    "batch_size = 128\n",
    "sample_interval = 2000\n",
    "\n",
    "d_losses, g_losses = [], []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Train Discriminator\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    real_imgs = X_train[idx]\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    fake_imgs = generator.predict(noise)\n",
    "    \n",
    "    d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    # Train Generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "    \n",
    "    # Store losses\n",
    "    d_losses.append(d_loss[0])\n",
    "    g_losses.append(g_loss)\n",
    "    \n",
    "    # Visualize progress\n",
    "    if epoch % sample_interval == 0 or epoch == 1:\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]:.4f}, acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\")\n",
    "        r, c = 2, 5\n",
    "        noise = np.random.normal(0, 1, (r*c, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5  # Scale to [0, 1]\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[i*c + j].reshape(28, 28), cmap='gray')\n",
    "                axs[i, j].axis('off')\n",
    "        plt.suptitle(f'Generated Images at Epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(d_losses, label=\"Discriminator loss\")\n",
    "plt.plot(g_losses, label=\"Generator loss\")\n",
    "plt.title(\"DCGAN Losses during Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33082eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
